---
title: "Dynamic Vector Tiles"
date: 2020-07-04T20:54:06-05:00
draft: true
---

# Dynamic Vector Tiles

- [Dynamic Vector Tiles](#dynamic-vector-tiles)
  - [Background](#background)
    - [Current Problem](#current-problem)
    - [Goals](#goals)
    - [Experiment Environment](#experiment-environment)
  - [PostgreSQL/PostGIS](#postgresqlpostgis)
    - [Vector Tile Generation](#vector-tile-generation)
    - [GDAL](#gdal)
    - [Data Loading](#data-loading)
      - [Spatial ETL](#spatial-etl)
      - [Post Processing](#post-processing)
  - [pg_tileserv](#pg_tileserv)
    - [Evaluation](#evaluation)
    - [Windows Installation](#windows-installation)
    - [Docker Installation](#docker-installation)
    - [AWS Installation](#aws-installation)
    - [Usage](#usage)
  - [Map Client Configuration](#map-client-configuration)
  - [Notes](#notes)
  - [Resources](#resources)

---

## Background

### Current Problem

- Vector tiles have to be pregenerated by ArcGIS
- Challenges
  - Large datasets
  - Lengthy processing
  - Must regenerate entire dataset to update
- PostGIS now has the ability to create vector tiles with the introduction of the ST_AsMVT function
  - Create at the database level
  - Offers the ability to create on-demand for map requests

### Goals

- Utilize cloud infrastructure
  - Serverless database
  - Functions as a service (FAAS)
- Use a large dataset of 100,000 features or more
- Use PostGIS for vector tile generation
- Identify a vector tile server
- Cache requested vector tiles

### Experiment Environment

1. PostgreSQL database with PostGIS extension
   - Serverless infrastructure
1. Dynamic vector tile server
   - Serverless infrastructure
1. GIS web client

---

## PostgreSQL/PostGIS

### Vector Tile Generation

Requirements

- PostGIS 2.4 needed for ST_AsMVT
- AWS Aurora supports PostGIS 2.4.4

Use `ST_TileEnvelope`, `ST_AsMVTGeom` and `ST_AsMVT` PostGIS functions to return vector tiles:

```sql
WITH mvtgeom AS (
  SELECT
    ST_AsMVTGeom(
      geom,
      ST_TileEnvelope(12, 513,412)) AS geom,
    featurename,
    description
  FROM points_of_interest
  WHERE ST_Intersects(
    geom,
    ST_TileEnvelope(12, 513,412))
  )
SELECT ST_AsMVT(mvtgeom.*) as mvt
FROM mvtgeom
```

Performance:

- `ST_AsMVTGeom` is expensive due to heavy geometry processing
- `ST_AsMVT` is a cheap operation. Since it's an aggregate it's parallelizable (Postgres 11-12). Postgres will create a new thread for each `ST_AsMVTGeom` geometry process in parallel. So you can spin up many threads to make performant.

  ![pg-parallizable](pg-parallizable.jpg)

Example:

- 100,000 parcel dataset in UTM 10
- Query which also includes reprojections
  ![pg-parcelquery](pg-parcelquery.jpg)
- Over 3000 results returned in 167 ms (using parallelization)
  ![pg-execution](pg-queryplan.jpg)

### GDAL

PostgreSQL data loading was accomplished with GDAL installed in WSL (Ubuntu Bionic). A Windows installation could also be used. The install instructions are for Linux.

These are instructions for installing GDAL on Ubuntu 18.04 (WSL). Anaconda can be used to install GDAL, but did not work in this case.

1. Run the following commands from a terminal:

   ```sh
   sudo add-apt-repository ppa:ubuntugis/ppa
   sudo apt-get update
   sudo apt-get gdal-bin
   ```

   If you run into errors accessing the personal package archive (PPA), continue on. Otherwise, skip to step 6.

1. Open _sources.list_ in an editor:

   ```bash
   sudo vi /etc/apt/sources.list
   ```

1. Add these lines to the end of the file:

   ```
   deb http://ppa.launchpad.net/ubuntugis/ppa/ubuntu bionic main
   deb-src http://ppa.launchpad.net/ubuntugis/ppa/ubuntu bionic main
   ```

1. Obtain the PPA public key.

   Go to [stable-release](https://launchpad.net/~ubuntugis/+archive/ubuntu/ppa). Under the _Technical Details_ section copy the **fingerprint** key. Example: _6B827C12C2D425E227EDCA75089EBE08314DF160_.

1. Add the key.

   ```bash
   sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 6B827C12C2D425E227EDCA75089EBE08314DF160
   ```

1. Update packages.

   ```bash
   sudo apt-get update
   ```

1. Perform install.

   ```bash
   sudo apt-get install gdal
   ```

---

### Data Loading

This section covers migrating parcels from a geodatabase into the PostgreSQL database.

#### Spatial ETL

1. Export parcels from geodatabase to a geopackage:

   ```sh
   ogr2ogr \
      -f GPKG parcel.gpkg \
      -progress \
      -skipfailures \
      -where "STDState = 'WY'" \
      "MSSQL:server=dev-sql-gis;driver=SQL Server Native Client 11.0;tables=arc.parcel;database=gis;trusted_connection=yes" arc.parcel
   ```

1. Create a data tunnel to serverless Aurora Postgres database in AWS:

   ```bash
   sudo ssh -N -L 5433:database-1.cluster-clyh9bfnfsjk.us-east-2.rds.amazonaws.com:5432 ec2-user@ec2-3-16-81-202.us-east-2.compute.amazonaws.com -i .ssh/aws-ec2.pem
   ```

1. Load parcels to PostreSQL from geopackage:

   ```sh
   ogr2ogr /
   -f PostgreSQL /
   PG:"host=127.0.0.1 port='5433' user='postgres' password='giBerItEArDePHYtO' dbname='geo'" /
   -progress /
   -skipfailures /
   -nln parcel /
   parcel.gpkg /
   arc.parcel
   ```

#### Post Processing

The PostgreSQL **parcel** table ended up not having a spatial reference or geometry type defined. Both are required by the tile server. These updates had to be applied:

1. Update SRID of all features in the geometry column

   ```sql
   SELECT UpdateGeometrySRID('parcel','geom',3857);
   ```

1. Set the geometry type

   ```sql
   ALTER TABLE parcel ALTER COLUMN geom type geometry(MultiPolygon, 3857) using ST_Multi(geom);
   ```

1. Excess columns were dropped for performance reasons when generating vector tiles on-the-fly.

   ```sql
   ALTER TABLE parcel
   --DROP COLUMN IF EXISTS objectid,
   DROP COLUMN IF EXISTS parcel_id,
   DROP COLUMN IF EXISTS fipscode,
   DROP COLUMN IF EXISTS apnunformatted,
   DROP COLUMN IF EXISTS apnsequencenumber,
   --DROP COLUMN IF EXISTS apnformatted,
   DROP COLUMN IF EXISTS apnoriginal,
   DROP COLUMN IF EXISTS accountnumber,
   DROP COLUMN IF EXISTS thomasbrosmapnumber,
   DROP COLUMN IF EXISTS mapreference2,
   DROP COLUMN IF EXISTS censustract,
   DROP COLUMN IF EXISTS blocknumber,
   DROP COLUMN IF EXISTS lotnumber,
   DROP COLUMN IF EXISTS range,
   DROP COLUMN IF EXISTS township,
   DROP COLUMN IF EXISTS section,
   DROP COLUMN IF EXISTS quartersection,
   DROP COLUMN IF EXISTS landuse,
   DROP COLUMN IF EXISTS hasmobilehome,
   DROP COLUMN IF EXISTS zoning,
   DROP COLUMN IF EXISTS propertyindicator,
   DROP COLUMN IF EXISTS subdivtractnumber,
   DROP COLUMN IF EXISTS subdivplatbook,
   DROP COLUMN IF EXISTS subdivplatpage,
   DROP COLUMN IF EXISTS subdivisionname,
   DROP COLUMN IF EXISTS ownercorporateindicator,
   DROP COLUMN IF EXISTS owner1lastname,
   DROP COLUMN IF EXISTS owner1firstname,
   DROP COLUMN IF EXISTS owner2lastname,
   DROP COLUMN IF EXISTS owner2firstname,
   DROP COLUMN IF EXISTS mailhousenumberprefix,
   DROP COLUMN IF EXISTS mailhousenumber,
   DROP COLUMN IF EXISTS mailhousenumber2,
   DROP COLUMN IF EXISTS mailhousenumbersuffix,
   DROP COLUMN IF EXISTS maildirection,
   DROP COLUMN IF EXISTS mailstreetname,
   DROP COLUMN IF EXISTS mailmode,
   DROP COLUMN IF EXISTS mailquadrant,
   DROP COLUMN IF EXISTS mailunitnumber,
   DROP COLUMN IF EXISTS mailcity,
   DROP COLUMN IF EXISTS mailstate,
   DROP COLUMN IF EXISTS mailzipcode,
   DROP COLUMN IF EXISTS mailcarriercode,
   DROP COLUMN IF EXISTS hasmailoptoutcode,
   DROP COLUMN IF EXISTS totalvaluecalculated,
   DROP COLUMN IF EXISTS landvaluecalculated,
   DROP COLUMN IF EXISTS improvementvaluecalculated,
   DROP COLUMN IF EXISTS totalvaluecalculatedindicator,
   DROP COLUMN IF EXISTS landvaluecalculatedindicator,
   DROP COLUMN IF EXISTS assessedtotalvalue,
   DROP COLUMN IF EXISTS assessedlandvalue,
   DROP COLUMN IF EXISTS assessedimprovementvalue,
   DROP COLUMN IF EXISTS markettotalvalue,
   DROP COLUMN IF EXISTS marketlandvalue,
   DROP COLUMN IF EXISTS marketimprovementvalue,
   DROP COLUMN IF EXISTS appraisedtotalvalue,
   DROP COLUMN IF EXISTS appraisedlandvalue,
   DROP COLUMN IF EXISTS appraisedimprovementvalue,
   DROP COLUMN IF EXISTS taxamount,
   DROP COLUMN IF EXISTS taxyear,
   DROP COLUMN IF EXISTS assessedyear,
   DROP COLUMN IF EXISTS taxarea,
   DROP COLUMN IF EXISTS documentnumber,
   DROP COLUMN IF EXISTS salerecordbookpage,
   DROP COLUMN IF EXISTS documenttype,
   DROP COLUMN IF EXISTS recordingdate,
   DROP COLUMN IF EXISTS saledate,
   DROP COLUMN IF EXISTS saleprice,
   DROP COLUMN IF EXISTS salecode,
   DROP COLUMN IF EXISTS sellername,
   DROP COLUMN IF EXISTS transactiontype,
   DROP COLUMN IF EXISTS titlecompanycode,
   DROP COLUMN IF EXISTS titlecompanyname,
   DROP COLUMN IF EXISTS isresidential,
   DROP COLUMN IF EXISTS firstmortgageamount,
   DROP COLUMN IF EXISTS mortgagedate,
   DROP COLUMN IF EXISTS mortgageloancode,
   DROP COLUMN IF EXISTS mortgagedeed,
   DROP COLUMN IF EXISTS mortgagetermcode,
   DROP COLUMN IF EXISTS mortgageterm,
   DROP COLUMN IF EXISTS mortgageduedate,
   DROP COLUMN IF EXISTS mortgageassumptionamount,
   DROP COLUMN IF EXISTS lendercode,
   DROP COLUMN IF EXISTS lendername,
   DROP COLUMN IF EXISTS secondmortgageamount,
   DROP COLUMN IF EXISTS secondmortgageloancode,
   DROP COLUMN IF EXISTS secondmortgagedeedtype,
   DROP COLUMN IF EXISTS frontfootage,
   DROP COLUMN IF EXISTS depthfootage,
   DROP COLUMN IF EXISTS landacres,
   DROP COLUMN IF EXISTS landsqft,
   DROP COLUMN IF EXISTS lotarea,
   DROP COLUMN IF EXISTS universalbuildingsqft,
   DROP COLUMN IF EXISTS buildingsqftindicator,
   DROP COLUMN IF EXISTS buildingsqft,
   DROP COLUMN IF EXISTS livingsqft,
   DROP COLUMN IF EXISTS groundfloorsqft,
   DROP COLUMN IF EXISTS grosssqft,
   DROP COLUMN IF EXISTS grosssqftadjusted,
   DROP COLUMN IF EXISTS basementsqft,
   DROP COLUMN IF EXISTS garageparkingsqft,
   DROP COLUMN IF EXISTS yearbuilt,
   DROP COLUMN IF EXISTS effectiveyearbuilt,
   DROP COLUMN IF EXISTS bedrooms,
   DROP COLUMN IF EXISTS totalrooms,
   DROP COLUMN IF EXISTS totalbathscalculated,
   DROP COLUMN IF EXISTS totalbaths,
   DROP COLUMN IF EXISTS fullbaths,
   DROP COLUMN IF EXISTS halfbaths,
   DROP COLUMN IF EXISTS quarterbaths,
   DROP COLUMN IF EXISTS threequarterbaths,
   DROP COLUMN IF EXISTS bathfixtures,
   DROP COLUMN IF EXISTS airconditioning,
   DROP COLUMN IF EXISTS basementfinish,
   DROP COLUMN IF EXISTS buildingcode,
   DROP COLUMN IF EXISTS buildingimprovementcode,
   DROP COLUMN IF EXISTS condition,
   DROP COLUMN IF EXISTS constructiontype,
   DROP COLUMN IF EXISTS exteriorwalls,
   DROP COLUMN IF EXISTS hasfireplace,
   DROP COLUMN IF EXISTS fireplacenumber,
   DROP COLUMN IF EXISTS fireplacetype,
   DROP COLUMN IF EXISTS foundation,
   DROP COLUMN IF EXISTS floor,
   DROP COLUMN IF EXISTS frame,
   DROP COLUMN IF EXISTS garage,
   DROP COLUMN IF EXISTS heating,
   DROP COLUMN IF EXISTS parkingspaces,
   DROP COLUMN IF EXISTS parkingtype,
   DROP COLUMN IF EXISTS haspool,
   DROP COLUMN IF EXISTS poolcode,
   DROP COLUMN IF EXISTS quality,
   DROP COLUMN IF EXISTS roofcover,
   DROP COLUMN IF EXISTS rooftype,
   DROP COLUMN IF EXISTS storycode,
   DROP COLUMN IF EXISTS storiesnumber,
   DROP COLUMN IF EXISTS buildingview,
   DROP COLUMN IF EXISTS locationinfluence,
   DROP COLUMN IF EXISTS buildingunits,
   DROP COLUMN IF EXISTS unitsnumber,
   DROP COLUMN IF EXISTS energyuse,
   DROP COLUMN IF EXISTS fuel,
   DROP COLUMN IF EXISTS sewer,
   DROP COLUMN IF EXISTS water,
   DROP COLUMN IF EXISTS legal1,
   DROP COLUMN IF EXISTS legal2,
   DROP COLUMN IF EXISTS legal3,
   DROP COLUMN IF EXISTS statefipscode,
   DROP COLUMN IF EXISTS countyfipscode,
   DROP COLUMN IF EXISTS createdby,
   DROP COLUMN IF EXISTS createdon,
   DROP COLUMN IF EXISTS updatedby,
   DROP COLUMN IF EXISTS updatedon,
   DROP COLUMN IF EXISTS apn,
   DROP COLUMN IF EXISTS apn2,
   DROP COLUMN IF EXISTS apn3,
   DROP COLUMN IF EXISTS address,
   DROP COLUMN IF EXISTS city,
   DROP COLUMN IF EXISTS zip,
   DROP COLUMN IF EXISTS zipplus,
   --DROP COLUMN IF EXISTS stdaddress,
   --DROP COLUMN IF EXISTS stdcity,
   --DROP COLUMN IF EXISTS stdstate,
   --DROP COLUMN IF EXISTS stdzip,
   --DROP COLUMN IF EXISTS stdzipplus,
   --DROP COLUMN IF EXISTS state,
   --DROP COLUMN IF EXISTS owner1fullname,
   --DROP COLUMN IF EXISTS owner2fullname,
   DROP COLUMN IF EXISTS gdb_geomattr_data;
   --DROP COLUMN IF EXISTS geom,
   ```

---

## pg_tileserv

### Evaluation

The best resource for all things vector tiles is the [awesome-vector-tiles](https://github.com/mapbox/awesome-vector-tiles) GitHub repo readme. The **Servers** section contains a comprehensive list of vector tile servers. However, many of these are static tile servers. These dynamic tile servers were chosen to be evaluated:

- pg_tileserv
- Tegola
- XYZ
- Cloud-tileserver

1. pg_tileserv

   A PostGIS-only tile server in **Go**. Strip away all the other requirements, it just has to take in HTTP tile requests and form and execute SQL.

   By restricting itself to only using PostGIS as a data source, pg_tileserv gains the following features:

   - Automatic configuration. The server can discover and automatically publish as tiles sources all tables it has read access to: just point it at a PostgreSQL/PostGIS database.
   - Full SQL flexibility. Using function layers, the server can run any SQL to generate tile outputs. Any data processing, feature filtering, or record aggregation that can be expressed in SQL, can be exposed as parameterized tile sources.
   - Database security model. You can restrict access to tables and functions using standard database access control. This means you can also use advanced access control techniques, like row-level security to dynamically filter access based on the login role.

   Architecture

   pg_tileserv is one component in “PostGIS for the Web” (aka “PostGIS FTW”), a growing family of Go spatial microservices. Database-centric applications naturally have a central source of coordinating state, the database, which allows otherwise independent microservices to coordinate and provide HTTP-level access to the database with less middleware software complexity.

   - **pg_tileserv** provides MVT tiles for interactive clients and smooth rendering
   - **pg_featureserv** provides GeoJSON feature services for reading and writing vector and attribute data from tables

   PostGIS for the Web makes it possible to stand up a spatial services architecture of stateless microservices surrounding a PostgreSQL/PostGIS database cluster, in a standard container environment, on any cloud platform or internal datacenter.

   [Docs](https://access.crunchydata.com/documentation/pg_tileserv/latest/)  
   [GitHub](https://github.com/CrunchyData/pg_tileserv)

1. Tegola

   Tegola is an open vector tile server written in Go delivering Mapbox Vector Tiles with support for PostGIS and GeoPackage data providers.

   Features

   - Native geometry processing (simplification, clipping, make valid, intersection, contains, scaling, translation)
   - Mapbox Vector Tile v2 specification compliant.
   - Embedded viewer with auto generated style for quick data visualization and inspection.
   - Support for PostGIS and GeoPackage data providers. Extensible design to support additional data providers.
   - Support for several cache backends: file, s3, redis, azure blob store.
   - Cache seeding and invalidation via individual tiles (ZXY), lat / lon bounds and ZXY tile list.
   - Parallelized tile serving and geometry processing.
   - Support for Web Mercator (3857) and WGS84 (4326) projections.
   - Support for AWS Lambda.

   [Docs](https://tegola.io)  
   [GitHub](https://github.com/go-spatial/tegola/tree/master/cmd/tegola_lambda)

1. XYZ

   A Node.js framework to develop web applications and APIs for spatial data. Developed by Geolytics.

   The XYZ framework is designed to serve spatial data from PostGIS data sources without the need of additional services. The framework is modular with dependencies on third party open source modules such as the open GIS engine Turf, the Leaflet javascript engine for interactive maps as well as the jsoneditor library used for the modification of JSON workspaces.

   XYZ is built with a PfaJn stack using Fastify as web server and JsRender for server side rendering of views.

   The code repository should work out of the box (zero-configuration) as serverless deployments with Zeit Now.

   [Docs](https://geolytix.gitbook.io/xyz-developer-guide/)

1. Cloud-Tileserver

   Serve mapbox vectortiles via AWS stack. Developed by an individual.

   [GitHub](https://github.com/henrythasler/cloud-tileserver)

**pg_tileserv** was chosen for the following reasons:

- Developed and supported by CrunchyData
- Exclusively written for Postgres/PostGIS to support "PostGIS for the Web"
- Stateless microservices
  - Server can automatically publish layers it has access to
  - Function layers support passing parameters for filtering
- Containerized
- Documentation
- pg_featureserv is an equivalent feature server for PostGIS

### Windows Installation

While the goal is to use Docker to be consistent with cloud deployments, installing pg_tileserv on Windows is straightforward to try out:

1. Download pg_tileserv [binaries](https://postgisftw.s3.amazonaws.com/pg_tileserv_latest_windows.zip).

1. Unzip the file, copy the pg_tileserv binary wherever you wish, or use it in place. If you move the binary, remember to move the assets/ directory to the same location, or start the server using the AssetsDir configuration option.

1. Open an SSH tunnel with bash.

   ```bash
   sudo ssh -N -L 5433:database-1.cluster-clyh9bfnfsjk.us-east-2.rds.amazonaws.com:5432 ec2-user@ec2-3-16-81-202.us-east-2.compute.amazonaws.com -i .ssh/aws-ec2.pem
   ```

1. Spin up pg_tileserv with a Windows Command Prompt.

   ```cmd
   SET DATABASE_URL=postgresql://postgres:giBerItEArDePHYtO@localhost:5433/geo
   pg_tileserv.exe
   ```

1. Launch http://localhost:7800.

### Docker Installation

Docker will be used for deploying pg_tileserv to AWS, therefore it is preferred to use Docker for local development as well.

These instructions will guide you on how to use Docker from both Windows and WSL. It is best practice to only run the Docker daemon in Windows, then configure WSL access. This will allow the Linux CLI to interface with the Windows service.

**Note:** wasn't able to install Docker [from a repository](https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-using-the-repository) (apt-get) due to a self-signed certificate issue. Instead, it was [installed from a Package](https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-from-a-package) which requires manual upates in the future.

1. Follow the [install instructions](https://docs.docker.com/docker-for-windows/install/) for Docker for Windows.

2. Open Docker for Windows Settings and expose Windows daemon to WSL:

   ![docker-daemon](docker-daemon.jpg)

   This is going to allow your local WSL instance to connect locally to the Docker daemon running within Docker for Windows.

3. Download latest versions of docker [packages](https://download.docker.com/linux/ubuntu/dists/bionic/pool/stable/amd64). Unsure if all three are needed, or just the CLI. The WSL Docker daemon will not be started.

   ```bash
   curl -kO https://download.docker.com/linux/ubuntu/dists/bionic/pool/stable/amd64/docker-ce-cli_19.03.8\~3-0\~ubuntu-bionic_amd64.deb
   ```

   ```bash
   curl -kO https://download.docker.com/linux/ubuntu/dists/bionic/pool/stable/amd64/containerd.io_1.2.6-3_amd64.deb
   ```

   ```bash
   curl -kO https://download.docker.com/linux/ubuntu/dists/bionic/pool/stable/amd64/docker-ce_19.03.8~3-0~ubuntu-bionic_amd64.deb
   ```

4. Install packages:

   ```bash
   sudo dpkg -i docker-ce-cli_19.03.8\~3-0\~ubuntu-bionic_amd64.deb
   sudo dpkg -i containerd.io_1.2.6-3_amd64.deb
   sudo dpkg -i docker-ce_19.03.8\~3-0\~ubuntu-bionic_amd64.deb
   ```

5. Allow your user to access the Docker CLI without needing root access.

   ```bash
   sudo usermod -aG docker $USER
   ```

6. Instruct the CLI to connect to the engine running under Windows.

   ```bash
   docker -H tcp://127.0.0.1:2375 images
   ```

7. Verify the Bash CLI is using the Windows engine. You should now be able to run docker commands from Bash.

   ```bash
   docker run hell-world
   ```

Run the Docker container:

1. Open an SSH tunnel.

   ```sh
   sudo ssh -N -L 5433:database-1.cluster-clyh9bfnfsjk.us-east-2.rds.amazonaws.com:5432 ec2-user@ec2-3-16-81-202.us-east-2.compute.amazonaws.com -i .ssh/aws-ec2.pem
   ```

1. Run pg_tileserv. The image will be downloaded automatically.

   ```sh
   # Reference
   # docker run -dt -e DATABASE_URL=postgres://user:pass@host/dbname -p 7800:7800 pramsey/pg_tileserv
   docker run -dt -e DATABASE_URL=postgres://postgres:giBerItEArDePHYtO@host.docker.internal:5433/geo -p 7800:7800 pramsey/pg_tileserv
   ```

   pg_tileserv will fire up in the container and connect to the AWS Aurora serverless PostgreSQL database through the SSH tunnel service on the container host. The container connects to the database service by specifying `host.docker.internal`, which resolves to the internal IP address used by the host.

1. Launch http://localhost:7800 which takes you to the service page.

When running the container in the future:

- Remove the `-p` values. Otherwise, an error will occurr indicating port 7800 is already allocated.

  ```sh
  # Remove -p 7800:7800
  docker run -dt -e DATABASE_URL=postgres://postgres:giBerItEArDePHYtO@host.docker.internal:5433/geo pramsey/pg_tileserv
  ```

- May need to delete your browser cache before going to http://localhost:7800.

### AWS Installation

**Push Docker Image Into an ECR Repository**

These steps describe how to create an Amazon ECR image repository, then push a local pg_tileserv Docker image to it. This is useful if customizations are made to the pg_tileserv image. Otherwise, this section can be skipped because AWS directly supports using the registered [image](https://hub.docker.com/r/pramsey/pg_tileserv) on Docker Hub.

1. Authenticate the Docker CLI to your default registry. This allows the `docker` command to push and pull images with Amazon ECR. For more info, see [Authenticate to Your Default Registry](https://docs.aws.amazon.com/AmazonECR/latest/userguide/getting-started-cli.html#cli-authenticate-registry).

   **Note:** This resulted in invalid certificate in chain errors. In WSL/Ubuntu the `--no-verify-ssl` option did not work to suppress. However, using the Windows CLI did.

   ```sh
   aws ecr get-login-password --no-verify-ssl --region us-east-2 | docker login --username AWS --password-stdin 275485693725.dkr.ecr.us-**east-2.amazonaws.com
   ```

1. Before you can push Docker images to ECR, you must create a repository to store them in:

   ```sh
   aws ecr create-repository --repository-name pg_tileserv --no-verify
   ```

   Output:

   ```json
   {
     "repository": {
       "repositoryArn": "arn:aws:ecr:us-east-2:275485693725:repository/pg_tileserv",
       "registryId": "275485693725",
       "repositoryName": "pg_tileserv",
       "repositoryUri": "275485693725.dkr.ecr.us-east-2.amazonaws.com/pg_tileserv",
       "createdAt": "2020-04-09T15:34:06-05:00",
       "imageTagMutability": "MUTABLE",
       "imageScanningConfiguration": {
         "scanOnPush": false
       }
     }
   }
   ```

1. Tag the image:

   ```sh
   docker -H tcp://127.0.0.1:2375 tag pramsey/pg_tileserv:latest 275485693725.dkr.ecr.us-east-2.amazonaws.com/pg_tileserv:latest
   ```

1. Push the image:

   ```sh
   docker -H tcp://127.0.0.1:2375 push 275485693725.dkr.ecr.us-east-2.amazonaws.com/pg_tileserv:latest
   ```

**Amazon Deployment**

AWS provides you with two options to run your containers:

1. Fargate
   > A _serverless_ compute engine for Amazon ECS that allows you to run Docker containers without having to manage servers or clusters. There is no need to scale, provision, or configure clusters of virtual machines to run containers. You don’t have to choose server types and how they communicate together.
1. EC2
   > Gives you similar capabilities to ECS managed by Fargate – in that you can deploy containerized workloads, declaratively specify their requirements, and let ECS do the placement and overall scheduling of them – but with full control over the compute environment those containers run in. Unlike Fargate, you manage the compute.

The instructions below are for **Fargate** deployments.

**Create a Cluster**

1. Open the Amazon ECS console at https://console.aws.amazon.com/ecs/.

1. From the navigation bar, select the Region to use.

1. In the navigation pane, choose Clusters.

1. On the Clusters page, choose Create Cluster.

1. Select the **Networking only...Powered by Fargate** template.

   <img src="cluster-fargate-template.jpg" height="75%" width="75%"/>

1. Click **Next step** button.

1. Enter a cluster name. Do not create a new VPC as we will use an existing one created for the Aurora PostgreSQL database.

   <img src="cluster-fargate-nameit.jpg" height="75%" width="75%"/>

1. Click the **Create** button.

1. Your new Fargate cluster is now listed in **Clusters**. Note that it won't have any services or running tasks yet.

   <img src="cluster-fargate-created.jpg" height="75%" width="75%"/>

**Create a Task Definition**

1. Click on **Task Definitions** in the ECS menu.

1. Click **Create new Task Definition**.

   <img src="taskdef-create.jpg" height="75%" width="75%"/>

1. Select the Fargate launch type and click the **Next step** button.

   <img src="taskdef-launch-type.jpg" height="75%" width="75%"/>

1. In the top **Configure task and container definitions** section, provide a name and set the **Task Role** to _ecsTaskExecution_ role.

   <img src="taskdef-container.jpg" height="60%" width="60%"/>

1. In the next section, set the **Task execution role** to _ecsTaskExcecutionRole_. Then, choose memory and CPU values based on your needs.

   <img src="taskdef-compute.jpg" height="60%" width="60%"/>

1. Now you will set the container up. Click the **Add a Container** button in the **Container Definitions** section.

1. In the **Standard** section, provide a container name. For **Image** enter _pramsey/pg_tileserv_ which will reference the Docker Hub registry. If you set up an ECR repository and pushed an image there, enter the url. Finally, add a port mapping for port 7800.

   <img src="taskdef-container-config.jpg" height="50%" width="50%"/>

1. There is one advanced configuration that needs to be made in the **Environment** section. Add a new key-value pair to tell pg_tileserv how to connect to the datase:

   **key:**: _DATABASE_URL_  
   **value:** _postgres://postgres:giBerItEArDePHYtO@database-1.cluster-clyh9bfnfsjk.us-east-2.rds.amazonaws.com:5432/geo_

   <img src="taskdef-container-config-env.jpg" height="60%" width="60%"/>

1. Click the **Add** button at the bottom of the page to finish. The new container definition is now listed.

   <img src="taskdef-container-added.jpg" height="60%" width="60%"/>

1. Click the **Create** button at the bottom of the page to complete the task definition. It will now show in the list of task definitions.

   <img src="taskdef-created.jpg" height="60%" width="60%"/>

**Create Service**

1. Navigate to **Clusters** in the ECS left menu, select the cluster you created earlier and click the **Create** button.

   <img src="service-create.jpg" height="70%" width="70%"/>

1. For **Step 1** select _FARGATE_ for **Launch type**. Choose the **Task Definition** from the dropdown that was previously created. It might have multiple revisions if it's been updated. Specify the **Cluster** the service will be run on and provide a service name. Set the number of tasks to a minimum 1.

   <img src="service-config.jpg" height="70%" width="70%"/>

   Finish **Step 1** by choosing the _Rolling update_ and clicking the **Next step** button.

   <img src="service-deployments.jpg" height="70%" width="70%"/>

1. **Step 2** involves network configuration. Select the VPC that the Aurora PostgreSQL database is running on and one or more subnets. It's also important to select/create a security group that defines inbound and outbound network rules that will allow access. Default values can be used for the rest of this step. Click the **Next step** button.

   <img src="service-network.jpg" height="70%" width="70%"/>

1. Leave Auto Scaling off in **Step 3**. Click **Next step**.

   <img src="service-scale.jpg" height="70%" width="70%"/>

1. Lastly, review your service and click the **Create** button which will open a Launch Status page. Then, click the **View Service** button.

   <img src="service-launch.jpg" height="70%" width="70%"/>

1. On the Service page take a look at the **Tasks** tab and find the status for the task. It will be ACTIVATING and then turns to a green RUNNING value when ready. If the task disappears from the list or there is an error status, go to the **Logs** tab and check for exceptions.

   <img src="service-activating.jpg" height="70%" width="70%"/>

   <img src="service-running.jpg" height="70%" width="70%"/>

**Use the Service**

You will now connect to the pg_tileserv instance.

1. First, determine the IP address to use. To find it, click the Task ID (664d17771-3c73...) link:

   <img src="service-running.jpg" height="70%" width="70%"/>

1. On the Task details page click the **ENI Id** link:

   <img src="address-eni.jpg" height="70%" width="70%"/>

1. The **IPv4 Public IP** value is the endpoint of the service:

   <img src="address-ip.jpg" height="70%" width="70%"/>

1. Open the pg_tileserv services page using the IP and specifying port 7800. For example, http://52.15.124.60:7800.

   <img src="services-page.jpg" height="50%" width="50%"/>

---

### Usage

**About**

pg_tileserv is one component in “PostGIS for the Web” (aka “PostGIS FTW”), a growing family of Go spatial microservices. Database-centric applications naturally have a central source of coordinating state, the database, which allows otherwise independent microservices to coordinate and provide HTTP-level access to the database with less middleware software complexity.

pg_tileserv works exclusively with PostGIS data. By restricting itself to only using PostGIS as a data source, pg_tileserv gains the following features:

- **Automatic configuration**. The server can discover and automatically publish as tiles sources all tables it has read access to: just point it at a PostgreSQL/PostGIS database.
- **Full SQL flexibility**. Using function layers, the server can run any SQL to generate tile outputs. Any data processing, feature filtering, or record aggregation that can be expressed in SQL, can be exposed as parameterized tile sources.
- **Database security model**. You can restrict access to tables and functions using standard database access control. This means you can also use advanced access control techniques, like row-level security to dynamically filter access based on the login role.

**Control Access**

By default, pg_tileserv will provide access to only those spatial tables and views that:

- your database connection has SELECT privileges for;
- include a geometry column;
- declare a geometry type; and,
- declare an SRID (spatial reference ID).

To restrict access to a certain set of tables, use database security principles:

- Create a role with limited privileges
- Only grant SELECT to that role for tables you want to publish
- Only grant EXECUTE to that role for functions you want to publish
- Connect pg_tileserv to the database using that role

**Tile Request Customization**

Most developers will use the tileurl as is, but it’s possible to add parameters to the URL to customize behaviour at run time:

- `limit` controls the number of features to write to a tile. The default is 50000

- `resolution` controls the resolution of a tile. The default is 4096 units per side for a tile.

- `buffer` controls the size of the extra data buffer for a tile. The default is 256 units.

- `properties` is a comma-separated list of properties to include in the tile. For wide tables with large numbers of columns, this allows a slimmer tile to be composed. For example: http://localhost:7800/public.ne_50m_admin_0_countries/{z}/{x}/{y}.pbf?limit=100000&properties=name,long_name

**Layer Types**

1. `Table layers` directly expose tables.
2. `Function layers` allow you create database functions for filtering and spatial analysis.

---

## Map Client Configuration

Map clients like MapBox GL, Leaflet, OpenLayers, and ArcGIS for JavaScript can render vector tiles generated by pg_tileserv. To add them, use the appropriate vector tile layer class in the API and set these properties:

1. **URL**: the endpoint to a pg*tileserv layer, which takes the form of \_http://host:7800/layer/{z}/{x}/{y}.pbf*.

1. **Style**: a JSON object conforming to the MapBox vector tile [style specification](https://docs.mapbox.com/mapbox-gl-js/style-spec/).

Simple examples for MapBox GL, Leaflet and OpenLayers can be found at the [pg_tileserv repo](https://github.com/CrunchyData/pg_tileserv/tree/master/examples).

Below is an example using the ArcGIS API for JavaScript, which can cut and pasted into the ArcGIS API for JavaScript [Sandbox](https://developers.arcgis.com/javascript/3/sandbox/sandbox.html).

Typically the _url_ for an ArcGIS vector tile layer is set to a style resource, which will contain the service url. This example demonstrates that you can set the service url and specify the style separately, proving compatibility with pg_tileserv and any othe tile server that follows the specification.

```javascript
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta
      name="viewport"
      content="initial-scale=1,maximum-scale=1,user-scalable=no"
    />
    <title>VectorTileLayer - 4.15</title>
    <style>
      html,
      body,
      #viewDiv {
        padding: 0;
        margin: 0;
        height: 100%;
        width: 100%;
      }
    </style>

    <link
      rel="stylesheet"
      href="https://js.arcgis.com/4.15/esri/themes/light/main.css"
    />
    <script src="https://js.arcgis.com/4.15/"></script>

    <script>
      require([
        "esri/Map",
        "esri/views/MapView",
        "esri/layers/VectorTileLayer"
      ], function(Map, MapView, VectorTileLayer) {
        // Create a Map
        var map = new Map();

        // Make map view and bind it to the map
        var view = new MapView({
          container: "viewDiv",
          map: map,
          center: [-98.5795, 39.8282],
          zoom: 3
        });

        var style = {
          "version": 8,
          "sprite": "https://cdn.arcgis.com/sharing/rest/content/items/4cf7e1fb9f254dcda9c8fbadb15cf0f8/resources/styles/../sprites/sprite",
          "glyphs": "https://basemaps.arcgis.com/arcgis/rest/services/World_Basemap_v2/VectorTileServer/resources/fonts/{fontstack}/{range}.pbf",
          "sources": {
          "esri": {
          "type": "vector",
          "url": "https://basemaps.arcgis.com/arcgis/rest/services/World_Basemap_v2/VectorTileServer"
          }
          },
          "layers": [{
             "id": "Land/Not ice",
             "type": "fill",
             "source": "esri",
             "source-layer": "Land",
             "filter": [
              "==",
              "_symbol",
              0
             ],
             "minzoom": 0,
             "layout": {},
             "paint": {
              "fill-color": "#f7f6d5"
             }
            }
           ]
          };

        var tileLayer = new VectorTileLayer({
          url:
            "https://basemaps.arcgis.com/v1/arcgis/rest/services/World_Basemap/VectorTileServer/tile/{z}/{y}/{x}.pbf",
            style: style
        });
        map.add(tileLayer);
      });
    </script>
  </head>
  <body>
    <div id="viewDiv"></div>
  </body>
</html>

```

---

## Notes

There are a few strategies for running a vector tile backend:

1. Pre generate all the tiles: In order to pre generate the entire planet you will need to consider the zoom range from 0–20 which will result in hundreds of billions of tiles being generated. Depending on the architecture of the system this will take from days to weeks to process. With that many files you will also need to start thinking about your file system capabilities or leveraging an object store like S3. Also note that when the data changes at least some part of the tile generation process will need to run again.

2. Serve tiles on demand: A vector tile server responds to tile requests on demand. The server will fetch data from a data provider, perform geo processing and then respond to the request. This strategy can work if data density is low and a good horizontal scaling strategy is in place. Without a scaling strategy the server will lock up and the user experience will quickly degrade. Additionally, without a caching strategy there’s a lot of unnecessary pressure on the database.

3. Pre generate and serve: Pre generate the coarser zooms (i.e. 0–12) and then serve the fine grained zooms (13–20+) on demand. Coupled with an object store (i.e. S3) for pre generated tiles and caching along with a content delivery network (i.e. Cloudfront) for edge caching, and a fairly robust backend is starting to take shape.

Unless you chose to pre generate all the tiles, the problem of managing a server cluster to handle traffic spikes still exists and so do the discussions around 0 downtime deployments, server upgrades, security patches, load balancing, etc.

Serverless PostgreSQL  
Standard PostgreSQL

Dynamic vector tiles  
Static vector tiles  
Hybrid vector tiles

Function layers would be great use of pg_tileserv (pg_featserv?) and serverless PostgreSQL

TODO:  
Function Table  
ArcGIS JSAPI

Pros/Cons

## Resources

PostGIS  
[ST_AsMVT](https://postgis.net/docs/ST_AsMVT.html)

AWS  
[Deploy Your Own Custom Docker Image on Amazon ECS](https://medium.com/underscoretec/deploy-your-own-custom-docker-image-on-amazon-ecs-b1584e62484)  
[How to deploy a Docker Container on AWS Elastic Container Service (ECS) using Elastic Container Registry (ECR)](https://towardsdatascience.com/how-to-deploy-a-docker-container-python-on-amazon-ecs-using-amazon-ecr-9c52922b738f)  
[Deploying Your First Docker Image to AWS](https://reflectoring.io/aws-deploy-docker-image-via-web-console/)  
[A beginner’s guide to Amazon’s Elastic Container Service](https://www.freecodecamp.org/news/amazon-ecs-terms-and-architecture-807d8c4960fd/)

Docker  
[Installing the Docker client on Windows Subsystem for Linux (Ubuntu)](https://medium.com/@sebagomez/installing-the-docker-client-on-ubuntus-windows-subsystem-for-linux-612b392a44c4)

pg_tileserv  
[Docs](https://access.crunchydata.com/documentation/pg_tileserv/1.0.1/)

Other  
[Serving Dynamic Vector Tiles](https://www.youtube.com/watch?v=t8eVmNwqh7M)  
[Vector Tile Specification](https://docs.mapbox.com/vector-tiles/specification/)  
[Serving Dynamic Vector Tiles from PostGIS](https://info.crunchydata.com/blog/dynamic-vector-tiles-from-postgis)
